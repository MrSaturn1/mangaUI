#!/usr/bin/env python3
"""
Improved character embedding system based on DiffSensei's dual-encoder approach
Addresses normalization, similarity, and architectural issues
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import CLIPModel, CLIPProcessor, ViTMAEModel, ViTImageProcessor
from PIL import Image
import numpy as np
from pathlib import Path
import json
from tqdm import tqdm


class DualEncoderEmbedder:
    """
    Improved character embedder following DiffSensei's dual-encoder approach:
    CLIP (general features) + ViT-MAE (manga-specific features) with proper normalization
    """
    
    def __init__(self, device=None):
        self.device = device or ("mps" if torch.backends.mps.is_available() 
                                else "cuda" if torch.cuda.is_available() else "cpu")
        
        # Load models
        print("Loading CLIP model...")
        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
        
        print("Loading ViT-MAE model (manga-specific features)...")
        self.vit_mae_model = ViTMAEModel.from_pretrained("facebook/vit-mae-base")
        self.vit_mae_processor = ViTImageProcessor.from_pretrained("facebook/vit-mae-base")
        
        # Move to device
        self.clip_model = self.clip_model.to(self.device)
        self.vit_mae_model = self.vit_mae_model.to(self.device)
        
        # Set to eval mode
        self.clip_model.eval()
        self.vit_mae_model.eval()
        
        # Get actual dimensions after loading models
        print("Checking model dimensions...")
        with torch.no_grad():
            test_image = torch.randn(1, 3, 224, 224).to(self.device)
            clip_test = self.clip_model.vision_model(test_image).last_hidden_state[:, 0]
            mae_test = self.vit_mae_model(test_image).last_hidden_state[:, 0]
            
            self.clip_dim = clip_test.shape[-1]
            self.mae_dim = mae_test.shape[-1]
            
        print(f"CLIP dimension: {self.clip_dim}")
        print(f"ViT-MAE dimension: {self.mae_dim}")
        
        # Feature fusion network with correct dimensions
        self.fusion_network = self._create_fusion_network().to(self.device)
        
        print(f"‚úÖ Dual encoder embedder initialized on {self.device}")
    
    def _create_fusion_network(self):
        """Create a simple fusion network to combine CLIP and ViT-MAE features"""
        total_dim = self.clip_dim + self.mae_dim
        return nn.Sequential(
            nn.Linear(total_dim, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 768),  # Output 768 for compatibility
            nn.LayerNorm(768)
        )
    
    def extract_clip_features(self, image):
        """Extract CLIP features with multiple layers for richer representation"""
        with torch.no_grad():
            # Process image
            inputs = self.clip_processor(images=image, return_tensors="pt").to(self.device)
            
            # Get vision features
            vision_outputs = self.clip_model.vision_model(**inputs, output_hidden_states=True)
            
            # Use multiple layers as DiffSensei does
            # Layer -1 (final): semantic features
            # Layer -3: balanced semantic + spatial
            final_features = vision_outputs.last_hidden_state[:, 0]  # CLS token
            mid_features = vision_outputs.hidden_states[-3][:, 0]    # Earlier layer
            
            # Combine features from multiple layers
            combined_clip = (final_features + mid_features) / 2
            
            # Normalize
            combined_clip = F.normalize(combined_clip, p=2, dim=-1)
            
            return combined_clip.squeeze(0)
    
    def extract_vit_mae_features(self, image):
        """Extract ViT-MAE features for manga-specific understanding"""
        with torch.no_grad():
            # Process image  
            inputs = self.vit_mae_processor(images=image, return_tensors="pt").to(self.device)
            
            # Get MAE features
            outputs = self.vit_mae_model(**inputs)
            
            # Use CLS token from last hidden state
            mae_features = outputs.last_hidden_state[:, 0]  # CLS token
            
            # Normalize
            mae_features = F.normalize(mae_features, p=2, dim=-1)
            
            return mae_features.squeeze(0)
    
    def add_character_specific_noise(self, features, character_name, strength=0.1):
        """Add character-specific deterministic variations to improve distinctiveness"""
        # Create character-specific seed from name
        char_seed = sum(ord(c) for c in character_name.upper())
        torch.manual_seed(char_seed)
        
        # Generate character-specific noise
        noise = torch.randn_like(features) * strength
        
        # Apply noise and renormalize
        modified_features = features + noise
        modified_features = F.normalize(modified_features, p=2, dim=-1)
        
        # Reset random seed
        torch.manual_seed(torch.initial_seed())
        
        return modified_features
    
    def extract_dual_embedding(self, image_path, character_name, enhance_distinctiveness=True):
        """
        Extract dual-encoder embedding following DiffSensei approach
        """
        try:
            # Load and preprocess image
            image = Image.open(image_path).convert("RGB")
            
            # Extract features from both encoders
            clip_features = self.extract_clip_features(image)
            mae_features = self.extract_vit_mae_features(image)
            
            # Combine features
            combined_features = torch.cat([clip_features, mae_features], dim=0)
            
            # Process through fusion network
            with torch.no_grad():
                fused_features = self.fusion_network(combined_features.unsqueeze(0)).squeeze(0)
            
            # Add character-specific variations for better distinctiveness
            if enhance_distinctiveness:
                fused_features = self.add_character_specific_noise(
                    fused_features, character_name, strength=0.15
                )
            
            # Final normalization - CRITICAL for model compatibility
            final_embedding = F.normalize(fused_features, p=2, dim=-1)
            
            # Verify normalization
            norm = torch.norm(final_embedding).item()
            if abs(norm - 1.0) > 0.001:
                print(f"‚ö†Ô∏è Warning: Embedding norm is {norm:.4f}, expected 1.0")
            
            return final_embedding.cpu()
            
        except Exception as e:
            print(f"‚ùå Error extracting embedding for {character_name}: {e}")
            # Return normalized random embedding as fallback
            fallback = torch.randn(768)
            return F.normalize(fallback, p=2, dim=-1)
    
    def test_embedding_quality(self, embeddings_dict):
        """Test the quality of generated embeddings"""
        print("\nüß™ TESTING EMBEDDING QUALITY")
        print("=" * 40)
        
        embeddings_list = list(embeddings_dict.values())
        char_names = list(embeddings_dict.keys())
        
        if len(embeddings_list) < 2:
            print("Need at least 2 embeddings to test")
            return
        
        # Stack embeddings
        all_embeddings = torch.stack(embeddings_list)
        
        # Check normalization
        norms = torch.norm(all_embeddings, dim=1)
        print(f"Embedding norms: min={norms.min():.4f}, max={norms.max():.4f}, mean={norms.mean():.4f}")
        
        # Check distinctiveness
        similarities = []
        for i in range(len(char_names)):
            for j in range(i+1, len(char_names)):
                sim = torch.cosine_similarity(embeddings_list[i], embeddings_list[j], dim=0)
                similarities.append(sim.item())
                print(f"Similarity {char_names[i]} vs {char_names[j]}: {sim:.4f}")
        
        avg_similarity = np.mean(similarities)
        print(f"\nAverage pairwise similarity: {avg_similarity:.4f}")
        
        if avg_similarity < 0.3:
            print("‚úÖ Excellent distinctiveness!")
        elif avg_similarity < 0.5:
            print("‚úÖ Good distinctiveness")
        elif avg_similarity < 0.7:
            print("‚ö†Ô∏è Moderate distinctiveness - could be better")
        else:
            print("üö® Poor distinctiveness - embeddings too similar!")
        
        return avg_similarity
    
    def generate_all_embeddings(self, keepers_dir="character_output/character_images/keepers", 
                               force_regenerate=False):
        """Generate improved embeddings for all characters"""
        keepers_path = Path(keepers_dir)
        embeddings_dir = Path("character_output/character_embeddings")
        embeddings_dir.mkdir(parents=True, exist_ok=True)
        
        if not keepers_path.exists():
            print(f"‚ùå Keepers directory not found: {keepers_path}")
            return {}
        
        # Get character images
        image_files = list(keepers_path.glob("*.png"))
        image_files = [img for img in image_files if not img.name.endswith("_card.png")]
        
        print(f"üìÅ Found {len(image_files)} character images")
        print("üîÑ Generating dual-encoder embeddings...")
        
        embeddings = {}
        embeddings_map = {}
        
        for image_path in tqdm(image_files, desc="Processing characters"):
            character_name = image_path.stem
            embedding_path = embeddings_dir / f"{character_name}.pt"
            
            if not force_regenerate and embedding_path.exists():
                print(f"‚è≠Ô∏è Embedding for {character_name} exists, loading...")
                try:
                    embedding = torch.load(embedding_path, map_location='cpu')
                    embeddings[character_name] = embedding
                    continue
                except:
                    print(f"‚ö†Ô∏è Failed to load {character_name}, regenerating...")
            
            # Generate new embedding
            embedding = self.extract_dual_embedding(image_path, character_name)
            
            # Save embedding
            torch.save(embedding, embedding_path)
            embeddings[character_name] = embedding
            
            # Update map
            embeddings_map[character_name] = {
                "name": character_name,
                "image_path": str(image_path),
                "embedding_path": str(embedding_path),
                "embedding_type": "dual_encoder_v2_768",
                "embedding_shape": list(embedding.shape),
                "features": {
                    "clip_large": True,
                    "vit_mae": True,
                    "dual_encoder": True,
                    "normalized": True,
                    "enhanced_distinctiveness": True
                }
            }
        
        # Test quality
        if len(embeddings) > 1:
            self.test_embedding_quality(embeddings)
        
        # Save embeddings map
        map_path = embeddings_dir / "character_embeddings_map.json"
        existing_map = {}
        if map_path.exists():
            with open(map_path, 'r') as f:
                existing_map = json.load(f)
        
        existing_map.update(embeddings_map)
        
        with open(map_path, 'w') as f:
            json.dump(existing_map, f, indent=2)
        
        print(f"\n‚úÖ Generated embeddings for {len(embeddings)} characters")
        print(f"üíæ Saved to: {embeddings_dir}")
        
        return embeddings


class EmbeddingNormalizer:
    """Utility to fix existing embeddings by normalizing them"""
    
    @staticmethod
    def normalize_existing_embeddings(embeddings_dir="character_output/character_embeddings"):
        """Normalize all existing embeddings to unit length"""
        embeddings_path = Path(embeddings_dir)
        
        if not embeddings_path.exists():
            print(f"‚ùå Embeddings directory not found: {embeddings_path}")
            return
        
        # Find all embedding files
        embedding_files = list(embeddings_path.glob("*.pt"))
        
        print(f"üîß NORMALIZING {len(embedding_files)} EMBEDDINGS")
        print("=" * 50)
        
        for embedding_file in tqdm(embedding_files, desc="Normalizing"):
            try:
                # Load embedding
                embedding = torch.load(embedding_file, map_location='cpu')
                original_norm = torch.norm(embedding).item()
                
                # Normalize
                normalized_embedding = F.normalize(embedding, p=2, dim=-1)
                new_norm = torch.norm(normalized_embedding).item()
                
                # Save normalized embedding
                torch.save(normalized_embedding, embedding_file)
                
                print(f"‚úÖ {embedding_file.stem}: {original_norm:.4f} ‚Üí {new_norm:.4f}")
                
            except Exception as e:
                print(f"‚ùå Error normalizing {embedding_file.stem}: {e}")
        
        print("\n‚úÖ NORMALIZATION COMPLETE!")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Generate improved character embeddings")
    parser.add_argument("--generate", action="store_true", 
                       help="Generate new dual-encoder embeddings")
    parser.add_argument("--normalize", action="store_true", 
                       help="Normalize existing embeddings")
    parser.add_argument("--test_quality", action="store_true", 
                       help="Test quality of existing embeddings")
    parser.add_argument("--force", action="store_true", 
                       help="Force regeneration of existing embeddings")
    
    args = parser.parse_args()
    
    if args.normalize:
        EmbeddingNormalizer.normalize_existing_embeddings()
    elif args.generate:
        embedder = DualEncoderEmbedder()
        embeddings = embedder.generate_all_embeddings(force_regenerate=args.force)
    elif args.test_quality:
        # Load and test existing embeddings
        embeddings_dir = Path("character_output/character_embeddings")
        embeddings = {}
        
        for file in embeddings_dir.glob("*.pt"):
            if file.name != "character_embeddings_map.json":
                embeddings[file.stem] = torch.load(file, map_location='cpu')
        
        if embeddings:
            embedder = DualEncoderEmbedder()
            embedder.test_embedding_quality(embeddings)
        else:
            print("No embeddings found to test")
    else:
        print("üéØ IMPROVED CHARACTER EMBEDDING SYSTEM")
        print("=" * 40)
        print("This system addresses the key issues identified:")
        print("‚úÖ Proper normalization (norm = 1.0)")
        print("‚úÖ Dual-encoder approach (CLIP + ViT-MAE)")
        print("‚úÖ Enhanced distinctiveness")
        print("‚úÖ Character-specific variations")
        print()
        print("Usage:")
        print("  --generate       Generate new embeddings")
        print("  --normalize      Fix existing embeddings")
        print("  --test_quality   Test embedding quality")
        print("  --force          Force regeneration")
        print()
        print("Quick fix for current embeddings:")
        print("  python improved_embedder.py --normalize")


if __name__ == "__main__":
    main()